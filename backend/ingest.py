import os
import glob
from langchain_community.document_loaders import TextLoader, DirectoryLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from db import get_vector_store

# é»˜è®¤çŸ¥è¯†åº“ç›®å½•ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼ŒåŒ…å«ç²¾åå†…å®¹ï¼‰
KNOWLEDGE_DIR = os.path.join(os.path.dirname(__file__), "knowledge")
# å¤–éƒ¨çŸ¥è¯†åº“ç›®å½•ï¼ˆäº§å“çŸ¥è¯†åº“ï¼‰
EXTERNAL_KNOWLEDGE_DIR = os.path.join(os.path.dirname(__file__), "..", "äº§å“çŸ¥è¯†åº“")
# PDF æ–‡ä»¶ç›®å½•
PDF_DIR = os.path.dirname(os.path.dirname(__file__))

# ä¼˜å…ˆåŠ è½½çš„ç²¾åæ–‡ä»¶ï¼ˆè¿™äº›æ–‡ä»¶ä¼šè¢«ä¼˜å…ˆæ£€ç´¢ï¼‰
PRIORITY_FILES = [
    "product_judge_essentials.md",  # ProductThink ä¸“ç”¨ç²¾å
    "lenny_podcast_essentials.md",  # Lennyæ’­å®¢ç²¾å
    "æ ¸å¿ƒçŸ¥è¯†ç²¾å.md",
    "äº§å“æ²‰æ€å½• Â· æ ¸å¿ƒçŸ¥è¯†ç²¾å.md",
    "æ¢å®äº§å“æ€ç»´æ¡†æ¶.txt",
    "ç²¾åæ´è§æ‘˜è¦.md",
]

# ç²¾åç›®å½•ï¼šæŒç»­æ›´æ–°ï¼ˆ01-06 ä¸»é¢˜ç›®å½•ï¼‰
ESSENTIAL_DIRS = [
    "01-äº§å“ä¸è®¾è®¡",
    "02-å•†ä¸šä¸æˆ˜ç•¥",
    "03-æ€ç»´ä¸è®¤çŸ¥",
    "04-æˆé•¿ä¸æ•ˆèƒ½",
    "05-æŠ€æœ¯ä¸AI",
    "06-å…¶ä»–",
]

def load_text_files(directory: str, priority_boost: bool = False):
    """åŠ è½½ç›®å½•ä¸­çš„ .txt å’Œ .md æ–‡ä»¶"""
    documents = []
    if not os.path.exists(directory):
        print(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
        return documents

    for ext in ["**/*.md", "**/*.txt"]:
        try:
            loader = DirectoryLoader(
                directory,
                glob=ext,
                loader_cls=TextLoader,
                loader_kwargs={"encoding": "utf-8"}
            )
            docs = loader.load()
            for doc in docs:
                doc.metadata["source_type"] = "text"
                doc.metadata["source_dir"] = directory
                # æ ‡è®°ä¼˜å…ˆçº§æ–‡ä»¶
                filename = os.path.basename(doc.metadata.get("source", ""))
                if filename in PRIORITY_FILES or priority_boost:
                    doc.metadata["priority"] = "high"
                else:
                    doc.metadata["priority"] = "normal"
            documents.extend(docs)
        except Exception as e:
            print(f"åŠ è½½ {ext} æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    return documents

def load_pdf_files(directory: str):
    """åŠ è½½ç›®å½•ä¸­çš„ PDF æ–‡ä»¶"""
    documents = []
    pdf_pattern = os.path.join(directory, "*.pdf")
    pdf_files = glob.glob(pdf_pattern)

    for pdf_path in pdf_files:
        try:
            print(f"æ­£åœ¨åŠ è½½ PDF: {os.path.basename(pdf_path)}")
            loader = PyPDFLoader(pdf_path)
            docs = loader.load()
            for doc in docs:
                doc.metadata["source_type"] = "pdf"
                doc.metadata["filename"] = os.path.basename(pdf_path)
                doc.metadata["priority"] = "normal"
            documents.extend(docs)
            print(f"  - åŠ è½½äº† {len(docs)} é¡µ")
        except Exception as e:
            print(f"åŠ è½½ PDF {pdf_path} æ—¶å‡ºé”™: {e}")

    return documents

def ingest_knowledge(mode: str = "full"):
    """
    è¯»å–çŸ¥è¯†åº“æ–‡ä»¶å¹¶ç´¢å¼•åˆ° ChromaDB

    mode:
    - "full": å¯¼å…¥æ‰€æœ‰å†…å®¹ï¼ˆå®Œæ•´æ¨¡å¼ï¼‰
    - "essentials": åªå¯¼å…¥ç²¾åå†…å®¹ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼Œæ¨èï¼‰
    """

    all_documents = []

    # 1. åŠ è½½å†…ç½®çŸ¥è¯†åº“ (backend/knowledge/) - ä¼˜å…ˆçº§æœ€é«˜
    print(f"\nğŸ“š åŠ è½½å†…ç½®çŸ¥è¯†åº“: {KNOWLEDGE_DIR}")
    if os.path.exists(KNOWLEDGE_DIR):
        docs = load_text_files(KNOWLEDGE_DIR, priority_boost=True)
        all_documents.extend(docs)
        print(f"  - åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰")

    if mode == "full":
        # 2. åŠ è½½å¤–éƒ¨çŸ¥è¯†åº“ (äº§å“çŸ¥è¯†åº“/)
        print(f"\nğŸ“š åŠ è½½å¤–éƒ¨çŸ¥è¯†åº“: {EXTERNAL_KNOWLEDGE_DIR}")
        if os.path.exists(EXTERNAL_KNOWLEDGE_DIR):
            docs = load_text_files(EXTERNAL_KNOWLEDGE_DIR)
            all_documents.extend(docs)
            print(f"  - åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£")

        # 3. åŠ è½½ PDF æ–‡ä»¶
        print(f"\nğŸ“š åŠ è½½ PDF æ–‡ä»¶: {PDF_DIR}")
        pdf_docs = load_pdf_files(PDF_DIR)
        all_documents.extend(pdf_docs)
        print(f"  - åŠ è½½äº† {len(pdf_docs)} é¡µ PDF")
    else:
        print("\nâš¡ ç²¾ç®€æ¨¡å¼ï¼šåŠ è½½ç²¾åæ–‡ä»¶ + ä¸»é¢˜ç›®å½•ï¼ˆæŒç»­æ›´æ–°ï¼‰")
        # 2. åŠ è½½å¤–éƒ¨çŸ¥è¯†åº“ä¸­çš„ä¸»é¢˜ç›®å½•ï¼ˆ01-06ï¼‰
        if os.path.exists(EXTERNAL_KNOWLEDGE_DIR):
            essential_docs = []
            for subdir in ESSENTIAL_DIRS:
                dir_path = os.path.join(EXTERNAL_KNOWLEDGE_DIR, subdir)
                if os.path.exists(dir_path):
                    docs = load_text_files(dir_path)
                    essential_docs.extend(docs)
            all_documents.extend(essential_docs)
            print(f"  - åŠ è½½äº† {len(essential_docs)} ä¸ªä¸»é¢˜æ–‡æ¡£")

    if not all_documents:
        print("\nâš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return

    print(f"\nğŸ“Š æ€»å…±åŠ è½½äº† {len(all_documents)} ä¸ªæ–‡æ¡£")

    # 4. åˆ†å‰²æ–‡æœ¬ï¼ˆä¼˜åŒ–åˆ†å—ç­–ç•¥ï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,  # å‡å°å—å¤§å°ï¼Œæé«˜æ£€ç´¢ç²¾åº¦
        chunk_overlap=150,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
    )
    docs = text_splitter.split_documents(all_documents)
    print(f"ğŸ“Š åˆ†å‰²æˆ {len(docs)} ä¸ªæ–‡æœ¬å—")

    # 5. ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“
    print("\nğŸ”„ æ­£åœ¨ç´¢å¼•åˆ° ChromaDB...")
    vector_store = get_vector_store()
    vector_store.add_documents(docs)
    print("âœ… çŸ¥è¯†åº“å¯¼å…¥å®Œæˆï¼")

    # 6. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\nğŸ“ˆ çŸ¥è¯†åº“ç»Ÿè®¡:")
    sources = {}
    priorities = {"high": 0, "normal": 0}
    for doc in all_documents:
        source_type = doc.metadata.get("source_type", "unknown")
        sources[source_type] = sources.get(source_type, 0) + 1
        priority = doc.metadata.get("priority", "normal")
        priorities[priority] = priorities.get(priority, 0) + 1

    for source_type, count in sources.items():
        print(f"  - {source_type}: {count} ä¸ªæ–‡æ¡£")
    print(f"  - é«˜ä¼˜å…ˆçº§æ–‡æ¡£: {priorities['high']} ä¸ª")
    print(f"  - æ™®é€šä¼˜å…ˆçº§æ–‡æ¡£: {priorities['normal']} ä¸ª")

def clear_knowledge():
    """æ¸…ç©ºçŸ¥è¯†åº“ï¼ˆç”¨äºé‡æ–°å¯¼å…¥ï¼‰"""
    import shutil
    db_dir = os.path.join(os.path.dirname(__file__), "chroma_db")
    if os.path.exists(db_dir):
        shutil.rmtree(db_dir)
        print("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
    else:
        print("âš ï¸ çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨")

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        if sys.argv[1] == "--clear":
            clear_knowledge()
        elif sys.argv[1] == "--essentials":
            clear_knowledge()
            ingest_knowledge(mode="essentials")
        elif sys.argv[1] == "--full":
            clear_knowledge()
            ingest_knowledge(mode="full")
    else:
        # é»˜è®¤ä½¿ç”¨ç²¾ç®€æ¨¡å¼
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python ingest.py --essentials  # åªå¯¼å…¥ç²¾åå†…å®¹ï¼ˆæ¨èï¼Œé€Ÿåº¦å¿«ï¼‰")
        print("  python ingest.py --full        # å¯¼å…¥æ‰€æœ‰å†…å®¹ï¼ˆå®Œæ•´ä½†è¾ƒæ…¢ï¼‰")
        print("  python ingest.py --clear       # æ¸…ç©ºçŸ¥è¯†åº“")
        print("\né»˜è®¤æ‰§è¡Œç²¾ç®€æ¨¡å¼...")
        clear_knowledge()
        ingest_knowledge(mode="essentials")
